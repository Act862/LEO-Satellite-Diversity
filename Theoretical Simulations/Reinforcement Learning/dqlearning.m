clear;clc;
env = rlPredefinedEnv("CartPole-Discrete");
obsInfo = getObservationInfo(env);
actInfo = getActionInfo(env);

statePath = [
    featureInputLayer(obsInfo.Dimension(1),"Normalization","none",...
    "Name","state")
    fullyConnectedLayer(128,'Name','fc1')
    reluLayer('Name','relu1')
    fullyConnectedLayer(128,'Name','fc2')
    reluLayer('Name','relu2')
    fullyConnectedLayer(numel(actInfo.Elements),'Name','output')
    ];
qNetwork = dlnetwork(layerGraph(statePath));

qRepresentation = rlQValueRepresentation(...
    qNetwork,obsInfo,actInfo,...
    'Observation',{'state'},'Action',{'output'});

agentOpts = rlDQNAgentOptions(...
    'UseDoubleDQN',true,...
    'TargetUpdateFrequency',4,...
    'ExperienceBufferLength',1e6,...
    'MiniBatchSize',64,...
    'DiscountFactor',0.99);
agentOpts.EpsilonGreedyExploration.Epsilon = 0.9;
agentOpts.EpsilonGreedyExploration.EpsilonMin = 0.05;
agentOpts.EpsilonGreedyExploration.EpsilonDecay = 1e-5;

agent = rlDQNAgent(qRepresentation,agentOpts);

trainOpts = rlTrainingOptions(...
    'MaxEpisodes',1000,...
    'MaxStepsPerEpisode',500,...
    'ScoreAveragingWindowLength',20,...
    'Verbose',true,...
    'Plots','training-progress',...
    'StopTrainingCriteria','AverageReward',...
    'StopTrainingValue',400);

trainingStats = train(agent,env,trainOpts);
save('dqnAgent.mat','agent','trainingStats');