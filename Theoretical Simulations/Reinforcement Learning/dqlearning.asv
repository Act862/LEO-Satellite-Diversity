clear;clc;
env = rlPredefinedEnv("CartPole-Discrete");
obsInfo = getObservationInfo(env);
actInfo = getActionInfo(env);

statePath = [
    featureInputLayer(obsInfo.Dimension(1),"Normalization","none",...
    "Name","state")
    fullyConnectedLayer(128,'Name','fc1')
    reluLayer('Name','relu1')
    fullyConnectedLayer(128,'Name','fc2')
    reluLayer('Name','relu2')
    fullyConnectedLayer(numel(actInfo.Elements),'Name','output')
    ];
qNetwork = dlnetwork(layerGraph(statePath));

qRepresentation = rlQValueRepresentation(...
    qNetwork,obsInfo,actInfo,...
    'Observation',{'state'},'Action',{'output'});

ege = EpsilonGreedyExploration('Epsilon',0.9,...
    'Epsi')

agentOpts = rlDQNAgentOptions(...
    'UseDoubleDQN',true,...
    'TargetUpdateFrequency',4,...
    'ExperienceBufferLength',1e6,...
    'MiniBatchSize',64,...
    'DiscountFactor',0.99,...
    'EpsilonGreedyExploration',struct('Epsilon',0.9,'EpsilonMin',...
    0.05,'EpsilonDecay',1e-5));

agent = rlDQNAgent(qRepresentation,agentOpts);

trainOpts = rlTrainingOptions(...
    'MaxEpisodes',1000,...
    'MaxStepsPerEpisode',500,...
    'ScoreAveragingWindowLength',20,...
    'Verbose',true,...
    'Plots','training-progress',...
    'StopTrainingCriteria','AverageReward',...
    'StopTrainingValue',400,...
    'ValidationFrequency',50,...
    'ValidationEpisodes',5);

trainingStats = train(agent,env,trainOpts);
save('dqnAgent.mat','agent','trainingStats');